{"architectures":["LlamaForCausalLM"],"hidden_size":256,"intermediate_size":1024,"num_hidden_layers":6,"num_attention_heads":8,"num_key_value_heads":8,"max_position_embeddings":2048,"rope_scaling":null}
