{"architectures":["LlamaForCausalLM"],"hidden_size":16,"intermediate_size":64,"num_hidden_layers":2,"num_attention_heads":4,"num_key_value_heads":4,"max_position_embeddings":2048,"rope_scaling":null}
